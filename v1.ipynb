{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "e454c106f5bac3ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, AutoTokenizer\n",
    "from datasets import load_dataset"
   ],
   "id": "9086b64b13f91d72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "WHISPER_MODEL_NAME = \"openai/whisper-base\"\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-3.2-3B\""
   ],
   "id": "9c975d197d51ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "whisper_processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)"
   ],
   "id": "89201f4b0f5d0a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset_name = \"gpt-omni/VoiceAssistant-400K\"",
   "id": "a1a636a956f029bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = load_dataset(dataset_name, split='train', streaming=True)",
   "id": "41f28eb5bb4b6bf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(torch.cuda.memory_summary())"
   ],
   "id": "b556867928bb62c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import WhisperProcessor, PreTrainedTokenizer\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class GPTVoiceAssistantDataCollator:\n",
    "    whisper_processor: WhisperProcessor\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    separator_token_id: int = 128000\n",
    "    base_index: int = 1616\n",
    "    switch_frequency: int = 4\n",
    "    required_sample_rate: int = 16000\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        audios = []\n",
    "        for sample in batch:\n",
    "            waveform = torch.tensor(sample[\"question_audio\"][\"array\"]).float()\n",
    "            orig_sr = sample[\"question_audio\"][\"sampling_rate\"]\n",
    "            if orig_sr != self.required_sample_rate:\n",
    "                waveform = torchaudio.functional.resample(waveform, orig_sr, self.required_sample_rate)\n",
    "            audios.append(waveform)\n",
    "\n",
    "        # Process audio with WhisperProcessor\n",
    "        audio_inputs = self.whisper_processor(\n",
    "            audios,\n",
    "            sampling_rate=self.required_sample_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_features = audio_inputs.input_features  # shape [B, 80, T]\n",
    "\n",
    "        # Tokenize text from the 'question' field\n",
    "        texts = [sample[\"question\"] for sample in batch]\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        # Prepend separator token\n",
    "        sep_token = torch.full((batch_size, 1), self.separator_token_id, dtype=input_ids.dtype)\n",
    "        input_ids = torch.cat([sep_token, input_ids], dim=1)\n",
    "\n",
    "        sep_mask = torch.ones((batch_size, 1), dtype=attention_mask.dtype)\n",
    "        attention_mask = torch.cat([sep_mask, attention_mask], dim=1)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n"
   ],
   "id": "55cf99ba2f88af14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ldpc = input_parameters = LibriSpeechDataCollator(\n",
    "    whisper_processor=whisper_processor,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "e8de04676e7b51f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "iterator = iter(dataset)\n",
    "batch = [next(iterator) for _ in range(3)]"
   ],
   "id": "3b297a469f94a4ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(batch[0].keys())",
   "id": "8812b25a69dc3a20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_parameters = ldpc(batch)",
   "id": "6c9d40b06b4eec7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(input_parameters[\"input_features\"].shape)\n",
    "print(input_parameters[\"labels\"].shape)\n",
    "print(input_parameters[\"input_ids\"].shape)\n",
    "print(input_parameters[\"attention_mask\"].shape)"
   ],
   "id": "4f6f72b9bc621c06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_parameters['input_features'] = input_parameters['input_features'].cuda(0).to(torch.bfloat16)\n",
    "input_parameters['labels'] = input_parameters['labels'].cuda(0)\n",
    "input_parameters['input_ids'] = input_parameters['input_ids'].cuda(0)\n",
    "input_parameters['attention_mask'] = input_parameters['attention_mask'].cuda(0)"
   ],
   "id": "5d4c3951ed1069c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from models import SpeechToTextModel",
   "id": "a0790e73edfa1510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SpeechToTextModel(\n",
    "    whisper_model_name=WHISPER_MODEL_NAME,\n",
    "    llama_model_name=LLAMA_MODEL_NAME,\n",
    "    hidden_dims=[2048, 1024, 2048, 1024, 2048],\n",
    "    train_whisper=False,\n",
    "    train_llama=False\n",
    ")\n",
    "model = model.to(torch.device(\"cuda:0\"), dtype=torch.bfloat16)"
   ],
   "id": "775984e08af2b20a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for param in model.parameters():\n",
    "    print(param.device)\n",
    "\n",
    "for input_id in input_parameters['labels']:\n",
    "    print(input_id.device)"
   ],
   "id": "715c77f83f90d253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outputs = model(\n",
    "    input_features=input_parameters['input_features'],\n",
    "    input_ids=input_parameters['input_ids'],\n",
    "    attention_mask=input_parameters['attention_mask'],\n",
    "    labels=input_parameters['labels'],\n",
    ")"
   ],
   "id": "a4d2dff14691181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "37c55e67601de802",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
