{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "e454c106f5bac3ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, AutoTokenizer\n",
    "from datasets import load_dataset"
   ],
   "id": "9086b64b13f91d72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "WHISPER_MODEL_NAME = \"openai/whisper-base\"\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-3.2-3B\""
   ],
   "id": "9c975d197d51ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "whisper_processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)"
   ],
   "id": "89201f4b0f5d0a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset_name = \"openslr/librispeech_asr\"",
   "id": "a1a636a956f029bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = load_dataset(dataset_name, 'clean', split='train.100', streaming=True)",
   "id": "41f28eb5bb4b6bf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(torch.cuda.memory_summary())"
   ],
   "id": "b556867928bb62c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import WhisperProcessor, PreTrainedTokenizer\n",
    "\n",
    "@dataclass\n",
    "class LibriSpeechDataCollator:\n",
    "    whisper_processor: WhisperProcessor\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    separator_token_id: int = 128000\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        audios = [sample['audio']['array'] for sample in batch]\n",
    "        texts = [sample['text'] for sample in batch]\n",
    "\n",
    "        # all libri speech are 16kHz\n",
    "        audio_inputs = self.whisper_processor(\n",
    "            audios,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_features = audio_inputs.input_features  # size [B, 80, 1500]\n",
    "        batch_size, seq_audio, _ = input_features.shape\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "\n",
    "        separator_token = torch.full((batch_size, 1), self.separator_token_id, dtype=input_ids.dtype)\n",
    "        input_ids_prepended = torch.cat([separator_token, input_ids], dim=1)\n",
    "\n",
    "        attend_to_separator = torch.full((batch_size, 1), 1, dtype=attention_mask.dtype)\n",
    "        attention_mask_prepended = torch.cat([attend_to_separator, attention_mask], dim=1)\n",
    "\n",
    "        labels = input_ids_prepended.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"input_ids\": input_ids_prepended,\n",
    "            \"attention_mask\": attention_mask_prepended,\n",
    "            \"labels\": labels\n",
    "        }"
   ],
   "id": "55cf99ba2f88af14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "iterator = iter(dataset)\n",
    "batch = [next(iterator) for _ in range(3)]"
   ],
   "id": "3b297a469f94a4ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ldpc = input_parameters = LibriSpeechDataCollator(\n",
    "    whisper_processor=whisper_processor,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "e8de04676e7b51f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_parameters = ldpc(batch)",
   "id": "6c9d40b06b4eec7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(input_parameters[\"input_features\"].shape)\n",
    "print(input_parameters[\"labels\"].shape)\n",
    "print(input_parameters[\"input_ids\"].shape)\n",
    "print(input_parameters[\"attention_mask\"].shape)"
   ],
   "id": "4f6f72b9bc621c06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_parameters['input_features'] = input_parameters['input_features'].cuda(0).to(torch.bfloat16)\n",
    "input_parameters['labels'] = input_parameters['labels'].cuda(0)\n",
    "input_parameters['input_ids'] = input_parameters['input_ids'].cuda(0)\n",
    "input_parameters['attention_mask'] = input_parameters['attention_mask'].cuda(0)"
   ],
   "id": "5d4c3951ed1069c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from models import SpeechToTextModel",
   "id": "a0790e73edfa1510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SpeechToTextModel(\n",
    "    whisper_model_name=WHISPER_MODEL_NAME,\n",
    "    llama_model_name=LLAMA_MODEL_NAME,\n",
    "    hidden_dims=[2048, 1024, 2048, 1024, 2048],\n",
    "    train_whisper=False,\n",
    "    train_llama=False\n",
    ")\n",
    "model = model.to(torch.device(\"cuda:0\"), dtype=torch.bfloat16)"
   ],
   "id": "775984e08af2b20a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for param in model.parameters():\n",
    "    print(param.device)\n",
    "\n",
    "for input_id in input_parameters['labels']:\n",
    "    print(input_id.device)"
   ],
   "id": "715c77f83f90d253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outputs = model(\n",
    "    input_features=input_parameters['input_features'],\n",
    "    input_ids=input_parameters['input_ids'],\n",
    "    attention_mask=input_parameters['attention_mask'],\n",
    "    labels=input_parameters['labels'],\n",
    ")"
   ],
   "id": "a4d2dff14691181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2d505befd2bff14a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Collator:\n",
    "  def __init__(self, tokenizer: PreTrainedTokenizer, whisper_processor):\n",
    "    self.whisper_processor = whisper_processor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.separator_token_id: int = 128000\n",
    "\n",
    "  def preprocess(self, batch):\n",
    "    audios = [sample['audio']['array'] for sample in batch]\n",
    "    texts = [sample['text'] for sample in batch]\n",
    "\n",
    "    # all libri speech are 16kHz\n",
    "    audio_inputs = self.whisper_processor(\n",
    "        audios,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_features = audio_inputs.input_features\n",
    "    batch_size, seq_audio, _ = input_features.shape\n",
    "\n",
    "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    tokenized = self.tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    input_ids = tokenized.input_ids\n",
    "\n",
    "    separator_token = torch.full((batch_size, 1), self.separator_token_id, dtype=input_ids.dtype)\n",
    "    input_ids_prepended = torch.cat([separator_token, input_ids], dim=1)\n",
    "\n",
    "    labels = input_ids_prepended.clone()\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"input_ids\": input_ids_prepended,\n",
    "        \"labels\": labels\n",
    "    }"
   ],
   "id": "7b79657c38485f85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "37c55e67601de802",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
